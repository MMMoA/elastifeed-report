\subsection{Implementierung der Warteschlange}

Besonders das generieren von Screenshot und PDF-Ansichten einer Webseite hat sich im Entwicklungsprozess als sehr rechenintensiv gestaltet.
Um die Resourcennutzung zu optimieren bietet der Processor die Möglichkeit, nur so viele Anfragen an den Scraper zu senden, wie auch Tabs verfügbar sind.
Bei Abfragen eines RSS Feeds fallen oft mehr Aufgaben an, als Tabs bei den Scrapern verfügbar sind.
Lösung dieses Problem ist eine Warteschlange, die mit Redis \cite{redis} realisiert wird.
In dieser werden in einer doppelt verketteten Liste mit \texttt{RPUSH} Aufgaben gespeichert und mit \texttt{BLPOP} herausgenommen.
Die \texttt{BLPOP} Funktion nimmt das erste Element aus der Liste heraus.
Sollte kein Element in der Liste sein blockiert die Funktion so lange, bis sie ein Element entnehmen kann.
Schematisch dargestellt ist der Prozess in dieser \hyperref[processor:image:processor]{Abbildung}

\begin{figure}[t]
  \centering
  \includegraphics[width=\linewidth]{images/processor.png}
  \caption{Abarbeiten von Scrap-Jobs}
  \label{processor:image:processor}
\end{figure}


\subsection{Abfragen von RSS Feeds}
Die RSS Feeds werden zentral in der Datenbank des Collectors gespeichert und sind über eine REST-API verfügbar.
Um nicht bei jedem aktualisieren der RSS-Feeds alle sich im Feed befindenden Dokumente zu indizieren, wird in Redis als Schlüssen-Wert-Paar der RSS Feed und der Zeitpunkt an dem er zuletzt abgefragt wurde gespeichert.
Der Zeitpunkt des letzten Abfragens wird an die RSS Komponente von Elastifeed weiter gegeben sodass nur neue Elemente zurück geliefert werden.
Anschließend werden die Elemente am Ende der Warteschlange hinzugefügt.
Der Processor koordiniert diesen Prozess einem konfigurierbaren Zeitabstand.

\begin{minted}[breaklines]{python}
for feed in parsed:
    # Get last parsed timestamp out of redis or assume it was scraped on 10.05.2019
    last_time_raw = await redis.execute("get", f"feed:{feed['link']}") or b"2019-05-10T00:00:00.000000+00:00"
    last_time = datetime.fromisoformat(last_time_raw.decode("UTF-8"))
    for job in await rss.get(feed["link"], last_time):

        # Add to queue
        await redis.execute("RPUSH", "queue:items", dumps(QueueElement(
            url=job.url,
            title=job.title,
            feed_url=feed["link"],
            categories=[],
            indexes=[f"user-{u['id']}" for u in feed["users"]]
        )))

    # Update timestamp
    await redis.execute("set", f"feed:{feed['link']}", datetime.now(timezone.utc).astimezone().isoformat())
\end{minted}
