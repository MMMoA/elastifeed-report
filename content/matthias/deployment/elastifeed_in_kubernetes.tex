\subsection{Elastifeed in Kubernetes}

Alle Komponenten des Elastifeed-Backends werden als Container ausgeliefert, welche in einem \ac{k8s} Cluster ausgeführt werden.
Für unsere Testzwecke verwenden wir die minimale \ac{k8s} Distribution k3s \cite{k3s}.
Neben der Master Instanz, die zeitgleich auch als Worker agiert, fügen sich die verbleibenden VMs als Worker Nodes in den Cluster ein.

\subsubsection{Skalieren der Komponenten}

Jeder Microservice ist in einem \ac{k8s} Deployment als YAML beschrieben:
\begin{minted}{yaml}
apiVersion: apps/v1
kind: Deployment
metadata:
  namespace: elastifeed
  name: es-scraper-deployment
  labels:
    app: es-scraper
\end{minted}
Der erste Tag \texttt{apiVersion: apps/v1} gibt an, welcher Endpunkt der \ac{k8s} API angesprochen werden soll, \texttt{kind} dass wir ein \texttt{Deployment} Objekt erstellen möchten.
Im folgenden \texttt{metadata} Abschnitt werden Attribute wie der Name des Objekts und seine Label gesetzt, die später zum einfachen Selektieren des Objekt oder einer Gruppe von Objekten helfen.

\begin{minted}{yaml}
spec:
  replicas: 10 # Number of concurrent running instances
  selector:
    matchLabels:
      app: es-scraper
\end{minted}

Im nächsten Schritt geben wir an, wie viele Instanzen des im folgenden definierten Pod laufen sollen und nach welchem Selektierungskriterium die Pods des Deployment identifiziert werden sollen.

\begin{minted}{yaml}
  template:
    metadata:
      labels:
        app: es-scraper
    spec:
      containers:
      - name: es-scraper
        image: quay.io/elastifeed/es-scraper:latest
        env:
        - name: S3_ENDPOINT
          value: http://es-s3.rocketlan.de/
        - name: S3_BUCKET_NAME
          value: elastifeed
        - name: API_BIND
          value: ":8000"
        - name: MERCURY_URL
          value: "http://es-extractor-service/mercury/url"
        envFrom:
        - secretRef:
          name: s3-credentials
        ports:
        - containerPort: 8000 # Exposed port
      - name: browserless
        image: browserless/chrome:latest
        env:
        - name: CONNECTION_TIMEOUT # Never timeout a connection.
          value: "-1"
\end{minted}

Der Pod des Scraper besteht aus zwei Containern - unser Microservice benötigt noch einen Chrome-Browser der im Headless Modus läuft.
Beide Container laufen unter der selben IP Adresse und können über das Loopback-Interface miteinander Kommunizieren.
Umgebungsvariablen die wir für die Konfiguration des Scraper benötigen werden unter dem \texttt{env} Tag als Liste beschrieben.

\subsubsection{Loadbalancing zwischen den laufenden Instanzen}
Das obige \texttt{Deployment} started die Pods, macht diese jedoch nicht verfügbar.
Es ist möglich, jeden Pod einzeln über seinen Namen oder direkt über seine IP Adresse anzusprechen, die sich aber ändern, sobald das Deployment aktualisiert wird.
Um einen zentralen Eingangspunkt für Anfragen bereitzustellen, benutzen wir ein \texttt{Service} Objekt.
Hier selektieren wir alle Pods des \texttt{Deployment} über die vorher gesetzten Label.
Der Service ist über seinen Namen im DNS oder eine einzelne, sich nicht ändernde IP Adresse erreichbar und leitet Anfragen an die dahinterliegenden Pods weiter.

\begin{minted}{yaml}
apiVersion: v1
kind: Service
metadata:
  namespace: elastifeed
  name: es-scraper-service
spec:
  selector:
    app: es-scraper
  ports:
    - rotocol: TCP 
      port: 80 # We map everything to port 80 so we can omit the port in our code
      targetPort: 8000
\end{minted}

\subsubsection{Speicherverwaltung}
Zum Speichern von gerenderten Bildern und PDFs verwenden wir das S3 Protokoll.
Eine Open-Source Implementation bietet das verteilte Dateisystem Ceph \cite{ceph}, welches über Rook \cite{rook} in einem \ac{k8s} Cluster deployt werden kann.
Nachdem Rook unseren Ceph-Cluster initialisiert hat, können wir mit einem \ac{k8s} Objet einen Objektspeicher für Elastifeed erstellen:

\begin{minted}{yaml}
apiVersion: ceph.rook.io/v1
kind: CephObjectStore
metadata:
  name: s3-store
  namespace: rook-ceph
spec:
  metadataPool:
    failureDomain: host
    replicated:
      size: 2
  dataPool:
    failureDomain: host
    erasureCoded:
      dataChunks: 2
      codingChunks: 1
  gateway:
    type: s3
    sslCertificateRef:
    port: 80
    securePort:
    instances: 1
    allNodes: false
\end{minted}

Diesen müssen wir im Anschluss noch von außen erreichbar machen:

\begin{minted}{yaml}
apiVersion: v1
kind: Service
metadata:
  name: rook-ceph-rgw-s3-store-external
  namespace: rook-ceph
  labels:
    app: rook-ceph-rgw
    rook_cluster: rook-ceph
    rook_object_store: s3-store
spec:
  ports:
  - name: rgw
    port: 80
    protocol: TCP
    targetPort: 80
  selector:
    app: rook-ceph-rgw
    rook_cluster: rook-ceph
    rook_object_store: es-store
  sessionAffinity: None
  type: NodePort
\end{minted}
